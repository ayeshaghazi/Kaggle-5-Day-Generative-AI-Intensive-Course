{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "collapsed": true,
        "id": "zUNSFWXFCPq3"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Copyright 2025 Google LLC."
      ],
      "metadata": {
        "id": "9HROL-z_CxXT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google import genai\n",
        "from google.genai import types\n",
        "from IPython.display import HTML, Markdown, display\n"
      ],
      "metadata": {
        "id": "cKsc2XfBCrlL"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.api_core import retry\n",
        "\n",
        "is_retriable = lambda e: (isinstance(e, genai.errors.APIError) and e.code in {429, 503})\n",
        "\n",
        "genai.models.Models.generate_content = retry.Retry(predicate=is_retriable)(genai.models.Models.generate_content)\n"
      ],
      "metadata": {
        "id": "bu-D62-FDONs"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Running your first prompt\n",
        "Testing to see if the API key is set up correctly by making a request.\n",
        "\n",
        "The Python SDK uses a ```client``` object to make requests to the API. The client lets you control which back-end to use (between the Gemini API and the Vertex AI) and handles authentication (the API key).\n",
        "\n",
        "The ```gemini-2.0-flash``` model has been selected here.\n"
      ],
      "metadata": {
        "id": "DqRyb7-qEUsH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "client = genai.Client(api_key=userdata.get('GOOGLE_API_KEY'))\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model = \"gemini-2.0-flash\",\n",
        "    contents = \"Tell me about the different types of cat breeds.\"\n",
        ")\n",
        "\n",
        "print(response.text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bs_eVB5vERrR",
        "outputId": "95faea4b-9ff4-4ef0-db09-c10be0681776"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Okay, let's dive into the fascinating world of cat breeds!  There are dozens, if not hundreds, of recognized (and unrecognized) cat breeds, each with its own unique set of characteristics.  Here's a breakdown, categorized for easier digestion:\n",
            "\n",
            "**I.  Longhaired Breeds:**\n",
            "\n",
            "*   **Persian:**  The epitome of long-haired luxury.  Known for their long, flowing coat, round faces, short noses, and sweet, gentle personalities.  Require significant grooming.  Prone to certain health issues due to their brachycephalic (short-nosed) structure.\n",
            "\n",
            "*   **Maine Coon:** The \"gentle giant.\" One of the largest domestic cat breeds.  Long, shaggy coat, bushy tail, tufted ears, and a rugged appearance. Known for their intelligence, playfulness, and friendly nature.  Relatively low-maintenance grooming for a longhaired cat.\n",
            "\n",
            "*   **Ragdoll:**  Known for their docile and floppy nature.  Semi-long, silky coat, striking blue eyes, and a tendency to go limp when held (hence the name).  Gentle and affectionate.\n",
            "\n",
            "*   **Norwegian Forest Cat:**  Resilient and adaptable.  Long, water-resistant double coat developed to withstand harsh Scandinavian climates.  Bushy tail, almond-shaped eyes. Intelligent and playful.\n",
            "\n",
            "*   **Himalayan:**  Essentially a Persian with Siamese markings.  Long coat, blue eyes, and pointed coloration (darker color on the points: face, ears, paws, and tail).  Shares the same grooming requirements and potential health concerns as Persians.\n",
            "\n",
            "*   **Birman:**  Striking blue eyes and a silky, medium-long coat.  Color-pointed pattern with distinctive white \"gloves\" on their paws.  Gentle and affectionate.\n",
            "\n",
            "*   **Turkish Angora:**  Elegant and graceful.  Long, silky coat, often white, but can come in other colors.  Intelligent, playful, and often described as \"dog-like\" in their devotion.\n",
            "\n",
            "*   **Siberian:** Similar to the Norwegian Forest Cat and Maine Coon, they also have triple layered coats that are hypoallergenic.\n",
            "\n",
            "**II.  Shorthaired Breeds:**\n",
            "\n",
            "*   **Siamese:**  One of the most recognizable breeds.  Short, sleek coat, striking blue almond-shaped eyes, and a color-pointed pattern.  Known for their vocalness, intelligence, and demanding personalities.\n",
            "\n",
            "*   **British Shorthair:**  Round face, dense, plush coat, and a sturdy build.  Calm, easygoing, and affectionate. Often described as \"teddy bear-like.\"\n",
            "\n",
            "*   **American Shorthair:**  A classic breed with a sturdy build, round face, and short, dense coat.  Adaptable and easygoing, making them great family pets.  Comes in a wide variety of colors and patterns.\n",
            "\n",
            "*   **Bengal:**  A relatively new breed with a distinctive spotted or marbled coat reminiscent of a leopard.  Energetic, intelligent, and playful. Requires plenty of stimulation.\n",
            "\n",
            "*   **Abyssinian:**  Known for their ticked coat (each hair has bands of different colors), giving them a shimmering appearance.  Energetic, intelligent, and curious.  Affectionate but independent.\n",
            "\n",
            "*   **Russian Blue:**  Distinctive plush, silvery-blue coat and bright green eyes.  Quiet, gentle, and somewhat shy.\n",
            "\n",
            "*   **Scottish Fold:**  Known for their folded ears (due to a genetic mutation).  Sweet, gentle, and playful.  Prone to certain health issues related to the ear cartilage.\n",
            "\n",
            "*   **Burmese:**  Sleek, muscular build and a short, glossy coat.  Affectionate, playful, and often described as \"dog-like.\"\n",
            "\n",
            "*   **Bombay:**  A sleek, black cat with striking copper or green eyes.  Affectionate, playful, and often referred to as a \"mini-panther.\"\n",
            "\n",
            "*   **Korat:** Silvery blue fur with green eyes. Their features include a heart-shaped head and large ears.\n",
            "\n",
            "**III.  Hairless/Near-Hairless Breeds:**\n",
            "\n",
            "*   **Sphynx:**  Known for their lack of fur (though they have a fine down).  Wrinkled skin, large ears, and a muscular build.  Affectionate, intelligent, and demanding of attention.  Require regular bathing to remove oil buildup on their skin and are susceptible to sunburn and cold temperatures.\n",
            "\n",
            "*   **Peterbald:** Similar to the Sphynx and are also hairless and require regular bathing.\n",
            "\n",
            "*   **Donskoy:** Known for being hairless, but some can be born with fur, but shed shortly after being born.\n",
            "\n",
            "**IV.  Rex Breeds (Curly-Coated):**\n",
            "\n",
            "*   **Cornish Rex:**  Curly, wavy coat that lacks guard hairs.  Slender build, large ears, and a playful, energetic personality.  Hypoallergenic.\n",
            "\n",
            "*   **Devon Rex:**  Looser, more tousled curls than the Cornish Rex.  Large ears, elfin face, and a playful, mischievous personality.  Hypoallergenic.\n",
            "\n",
            "*   **Selkirk Rex:**  A breed with a naturally curly coat, including whiskers!  Can be long or short-haired.  Affectionate and tolerant.\n",
            "\n",
            "**V.  Other Notable Breeds:**\n",
            "\n",
            "*   **Manx:**  Known for their taillessness (or very short tail).  Sturdy build, round face, and a playful, intelligent personality.\n",
            "\n",
            "*   **Japanese Bobtail:**  Characterized by a short, pom-pom-like tail.  Energetic, playful, and intelligent.\n",
            "\n",
            "*   **Ocicat:**  Resembles a wild ocelot due to its spotted coat, but is entirely domestic.  Intelligent, playful, and affectionate.\n",
            "\n",
            "*   **Savannah:** A hybrid breed with a Serval (a wild African cat).  Tall, slender, and spotted.  Energetic, intelligent, and require a lot of space and enrichment.  Often have restrictions on ownership depending on local laws.\n",
            "\n",
            "**Important Considerations When Choosing a Breed:**\n",
            "\n",
            "*   **Temperament:**  Think about what you're looking for in a cat's personality.  Do you want a lap cat, a playful cat, or a more independent cat?\n",
            "*   **Grooming:**  Longhaired breeds require daily grooming to prevent mats and tangles.  Shorthaired breeds require less grooming.\n",
            "*   **Health:**  Some breeds are prone to certain health issues.  Research potential health problems before choosing a breed.\n",
            "*   **Activity Level:**  Some breeds are more active than others.  Consider your lifestyle and whether you can provide enough exercise and stimulation for a high-energy cat.\n",
            "*   **Cost:**  The cost of a purebred cat can vary significantly depending on the breed and the breeder.  Also, factor in the cost of food, vet care, and other expenses.\n",
            "*   **Ethical Breeding:** Always get a cat from a reputable breeder who prioritizes the health and well-being of their cats.  Avoid kitten mills and backyard breeders.  Consider adopting a cat from a shelter or rescue organization.  Many wonderful cats are waiting for loving homes!\n",
            "\n",
            "**Beyond Breed:**\n",
            "\n",
            "It's important to remember that breed is just one factor in a cat's personality.  Each cat is an individual, and their personality can be influenced by their environment, experiences, and upbringing.  Many wonderful mixed-breed cats can be found in shelters and rescue organizations.\n",
            "\n",
            "This list is not exhaustive, but it covers many of the more common and well-known cat breeds. Do your research and find the purrfect feline friend for you!\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This response is in the Markdown format. We render directly in the notebook."
      ],
      "metadata": {
        "id": "JhQZTO6PF8Gz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Markdown(response.text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "p35yOtEAFsI4",
        "outputId": "c5ecd8d6-50b6-48b2-9c8e-1f0e40086020"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Okay, let's dive into the fascinating world of cat breeds!  There are dozens, if not hundreds, of recognized (and unrecognized) cat breeds, each with its own unique set of characteristics.  Here's a breakdown, categorized for easier digestion:\n\n**I.  Longhaired Breeds:**\n\n*   **Persian:**  The epitome of long-haired luxury.  Known for their long, flowing coat, round faces, short noses, and sweet, gentle personalities.  Require significant grooming.  Prone to certain health issues due to their brachycephalic (short-nosed) structure.\n\n*   **Maine Coon:** The \"gentle giant.\" One of the largest domestic cat breeds.  Long, shaggy coat, bushy tail, tufted ears, and a rugged appearance. Known for their intelligence, playfulness, and friendly nature.  Relatively low-maintenance grooming for a longhaired cat.\n\n*   **Ragdoll:**  Known for their docile and floppy nature.  Semi-long, silky coat, striking blue eyes, and a tendency to go limp when held (hence the name).  Gentle and affectionate.\n\n*   **Norwegian Forest Cat:**  Resilient and adaptable.  Long, water-resistant double coat developed to withstand harsh Scandinavian climates.  Bushy tail, almond-shaped eyes. Intelligent and playful.\n\n*   **Himalayan:**  Essentially a Persian with Siamese markings.  Long coat, blue eyes, and pointed coloration (darker color on the points: face, ears, paws, and tail).  Shares the same grooming requirements and potential health concerns as Persians.\n\n*   **Birman:**  Striking blue eyes and a silky, medium-long coat.  Color-pointed pattern with distinctive white \"gloves\" on their paws.  Gentle and affectionate.\n\n*   **Turkish Angora:**  Elegant and graceful.  Long, silky coat, often white, but can come in other colors.  Intelligent, playful, and often described as \"dog-like\" in their devotion.\n\n*   **Siberian:** Similar to the Norwegian Forest Cat and Maine Coon, they also have triple layered coats that are hypoallergenic.\n\n**II.  Shorthaired Breeds:**\n\n*   **Siamese:**  One of the most recognizable breeds.  Short, sleek coat, striking blue almond-shaped eyes, and a color-pointed pattern.  Known for their vocalness, intelligence, and demanding personalities.\n\n*   **British Shorthair:**  Round face, dense, plush coat, and a sturdy build.  Calm, easygoing, and affectionate. Often described as \"teddy bear-like.\"\n\n*   **American Shorthair:**  A classic breed with a sturdy build, round face, and short, dense coat.  Adaptable and easygoing, making them great family pets.  Comes in a wide variety of colors and patterns.\n\n*   **Bengal:**  A relatively new breed with a distinctive spotted or marbled coat reminiscent of a leopard.  Energetic, intelligent, and playful. Requires plenty of stimulation.\n\n*   **Abyssinian:**  Known for their ticked coat (each hair has bands of different colors), giving them a shimmering appearance.  Energetic, intelligent, and curious.  Affectionate but independent.\n\n*   **Russian Blue:**  Distinctive plush, silvery-blue coat and bright green eyes.  Quiet, gentle, and somewhat shy.\n\n*   **Scottish Fold:**  Known for their folded ears (due to a genetic mutation).  Sweet, gentle, and playful.  Prone to certain health issues related to the ear cartilage.\n\n*   **Burmese:**  Sleek, muscular build and a short, glossy coat.  Affectionate, playful, and often described as \"dog-like.\"\n\n*   **Bombay:**  A sleek, black cat with striking copper or green eyes.  Affectionate, playful, and often referred to as a \"mini-panther.\"\n\n*   **Korat:** Silvery blue fur with green eyes. Their features include a heart-shaped head and large ears.\n\n**III.  Hairless/Near-Hairless Breeds:**\n\n*   **Sphynx:**  Known for their lack of fur (though they have a fine down).  Wrinkled skin, large ears, and a muscular build.  Affectionate, intelligent, and demanding of attention.  Require regular bathing to remove oil buildup on their skin and are susceptible to sunburn and cold temperatures.\n\n*   **Peterbald:** Similar to the Sphynx and are also hairless and require regular bathing.\n\n*   **Donskoy:** Known for being hairless, but some can be born with fur, but shed shortly after being born.\n\n**IV.  Rex Breeds (Curly-Coated):**\n\n*   **Cornish Rex:**  Curly, wavy coat that lacks guard hairs.  Slender build, large ears, and a playful, energetic personality.  Hypoallergenic.\n\n*   **Devon Rex:**  Looser, more tousled curls than the Cornish Rex.  Large ears, elfin face, and a playful, mischievous personality.  Hypoallergenic.\n\n*   **Selkirk Rex:**  A breed with a naturally curly coat, including whiskers!  Can be long or short-haired.  Affectionate and tolerant.\n\n**V.  Other Notable Breeds:**\n\n*   **Manx:**  Known for their taillessness (or very short tail).  Sturdy build, round face, and a playful, intelligent personality.\n\n*   **Japanese Bobtail:**  Characterized by a short, pom-pom-like tail.  Energetic, playful, and intelligent.\n\n*   **Ocicat:**  Resembles a wild ocelot due to its spotted coat, but is entirely domestic.  Intelligent, playful, and affectionate.\n\n*   **Savannah:** A hybrid breed with a Serval (a wild African cat).  Tall, slender, and spotted.  Energetic, intelligent, and require a lot of space and enrichment.  Often have restrictions on ownership depending on local laws.\n\n**Important Considerations When Choosing a Breed:**\n\n*   **Temperament:**  Think about what you're looking for in a cat's personality.  Do you want a lap cat, a playful cat, or a more independent cat?\n*   **Grooming:**  Longhaired breeds require daily grooming to prevent mats and tangles.  Shorthaired breeds require less grooming.\n*   **Health:**  Some breeds are prone to certain health issues.  Research potential health problems before choosing a breed.\n*   **Activity Level:**  Some breeds are more active than others.  Consider your lifestyle and whether you can provide enough exercise and stimulation for a high-energy cat.\n*   **Cost:**  The cost of a purebred cat can vary significantly depending on the breed and the breeder.  Also, factor in the cost of food, vet care, and other expenses.\n*   **Ethical Breeding:** Always get a cat from a reputable breeder who prioritizes the health and well-being of their cats.  Avoid kitten mills and backyard breeders.  Consider adopting a cat from a shelter or rescue organization.  Many wonderful cats are waiting for loving homes!\n\n**Beyond Breed:**\n\nIt's important to remember that breed is just one factor in a cat's personality.  Each cat is an individual, and their personality can be influenced by their environment, experiences, and upbringing.  Many wonderful mixed-breed cats can be found in shelters and rescue organizations.\n\nThis list is not exhaustive, but it covers many of the more common and well-known cat breeds. Do your research and find the purrfect feline friend for you!\n"
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Start a chat\n",
        "The previous example uses a single-turn, text-in/text-out structure. We can also set up multi-turn chat structure."
      ],
      "metadata": {
        "id": "kGTTMEJmGJY2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a new chat session using the Gemini-2.0-flash model. The history is an empty list and the chat starts without any prior conversation context\n",
        "chat = client.chats.create(model=\"gemini-2.0-flash\", history=[])\n",
        "response = chat.send_message(\"Hello! My name is Ayesha. I am a cat lover.\")\n",
        "print(response.text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LlYMhcO0GNPl",
        "outputId": "e706bdda-21da-4f28-d7b9-33cdcd9a7372"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello Ayesha! It's nice to meet you. As a large language model, I don't have personal preferences like loving cats, but I can definitely understand why you do! They're wonderful creatures.\n",
            "\n",
            "Is there anything I can help you with today, fellow cat enthusiast? Perhaps you'd like to talk about cat breeds, cat care tips, or maybe you need a poem written about a feline friend? Just let me know!\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = chat.send_message('Can you tell me something interesting about cats?')\n",
        "print(response.text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CG6BYx72HKlu",
        "outputId": "d0abe794-49bb-43e9-f9ad-ccb6dd038f25"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Okay, here's an interesting fact about cats that you might not know:\n",
            "\n",
            "**A cat's nose print is as unique as a human fingerprint!**\n",
            "\n",
            "That's right! Just like our fingerprints, the ridges and bumps on a cat's nose form a unique pattern. This means that no two cats (even identical twins) have the same nose print. So, if you wanted to, you could theoretically use a cat's nose print for identification purposes!\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Markdown(response.text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 110
        },
        "id": "r14xpDqqHfle",
        "outputId": "5399b061-6628-44e9-ab97-a948f8acaf44"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Okay, here's an interesting fact about cats that you might not know:\n\n**A cat's nose print is as unique as a human fingerprint!**\n\nThat's right! Just like our fingerprints, the ridges and bumps on a cat's nose form a unique pattern. This means that no two cats (even identical twins) have the same nose print. So, if you wanted to, you could theoretically use a cat's nose print for identification purposes!\n"
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "While the ```chat``` object is alive, the conversation state persists."
      ],
      "metadata": {
        "id": "M6_6pOuXIFZ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response = chat.send_message(\"Great! Do you remember what my name is?\")\n",
        "print(response.text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rMXdWAQfHiz3",
        "outputId": "4c8cef69-e58e-4dd7-eb6e-b3fcba1489df"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Yes, I do! Your name is Ayesha.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Choosing a model\n",
        "\n",
        "The Gemini API provides access to a number of models from the Gemini model family. We are going to use the API to list all of the available models."
      ],
      "metadata": {
        "id": "7hEDs3C3IPAX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for model in client.models.list():\n",
        "  print(model.name)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OmSN9f4HHsjs",
        "outputId": "cc8b6aad-837c-4f91-ec1d-ac23301660a6"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "models/chat-bison-001\n",
            "models/text-bison-001\n",
            "models/embedding-gecko-001\n",
            "models/gemini-1.0-pro-vision-latest\n",
            "models/gemini-pro-vision\n",
            "models/gemini-1.5-pro-latest\n",
            "models/gemini-1.5-pro-001\n",
            "models/gemini-1.5-pro-002\n",
            "models/gemini-1.5-pro\n",
            "models/gemini-1.5-flash-latest\n",
            "models/gemini-1.5-flash-001\n",
            "models/gemini-1.5-flash-001-tuning\n",
            "models/gemini-1.5-flash\n",
            "models/gemini-1.5-flash-002\n",
            "models/gemini-1.5-flash-8b\n",
            "models/gemini-1.5-flash-8b-001\n",
            "models/gemini-1.5-flash-8b-latest\n",
            "models/gemini-1.5-flash-8b-exp-0827\n",
            "models/gemini-1.5-flash-8b-exp-0924\n",
            "models/gemini-2.5-pro-exp-03-25\n",
            "models/gemini-2.0-flash-exp\n",
            "models/gemini-2.0-flash\n",
            "models/gemini-2.0-flash-001\n",
            "models/gemini-2.0-flash-exp-image-generation\n",
            "models/gemini-2.0-flash-lite-001\n",
            "models/gemini-2.0-flash-lite\n",
            "models/gemini-2.0-flash-lite-preview-02-05\n",
            "models/gemini-2.0-flash-lite-preview\n",
            "models/gemini-2.0-pro-exp\n",
            "models/gemini-2.0-pro-exp-02-05\n",
            "models/gemini-exp-1206\n",
            "models/gemini-2.0-flash-thinking-exp-01-21\n",
            "models/gemini-2.0-flash-thinking-exp\n",
            "models/gemini-2.0-flash-thinking-exp-1219\n",
            "models/learnlm-1.5-pro-experimental\n",
            "models/gemma-3-27b-it\n",
            "models/embedding-001\n",
            "models/text-embedding-004\n",
            "models/gemini-embedding-exp-03-07\n",
            "models/gemini-embedding-exp\n",
            "models/aqa\n",
            "models/imagen-3.0-generate-002\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The ```models.list``` response also returns additional information about the model's capabilities, like the token limits and supported parameters."
      ],
      "metadata": {
        "id": "5n0egZleIld5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The pprint module is used to pretty print Python data structures like dictionaries, lists, and other objects.\n",
        "# It formats the output to make it more readable, especially for complex or nested structures.\n",
        "\n",
        "from pprint import pprint\n",
        "\n",
        "for model in client.models.list():\n",
        "  if model.name == \"models/gemini-2.0-flash\":\n",
        "    pprint(model.to_json_dict())\n",
        "    break\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jtYbt5O6IjPn",
        "outputId": "a1274c7c-2c31-409b-cc9c-6169473f0e0a"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'description': 'Gemini 2.0 Flash',\n",
            " 'display_name': 'Gemini 2.0 Flash',\n",
            " 'input_token_limit': 1048576,\n",
            " 'name': 'models/gemini-2.0-flash',\n",
            " 'output_token_limit': 8192,\n",
            " 'supported_actions': ['generateContent', 'countTokens'],\n",
            " 'tuned_model_info': {},\n",
            " 'version': '2.0'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exploring generation parameters\n",
        "\n",
        "\n",
        "\n",
        "*   **Output length**:  When generating text with an LLM, the output length affects cost and performance. Generating more tokens increases computation, leading to higher energy consumption, latency, and cost. To stop the model from generating tokens past a limit, we can specify the `max_output_tokens` parameter when using the Gemini API. Specifying this parameter does not influence the generation of the output tokens, so the output will not become more stylistically or textually succinct, but it will stop generating tokens once the specified length is reached. Prompt engineering may be required to generate a more complete output for the given limit.\n",
        "\n"
      ],
      "metadata": {
        "id": "jt7y0gNgJl_S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.genai import types\n",
        "\n",
        "short_config = types.GenerateContentConfig(max_output_tokens=200)\n"
      ],
      "metadata": {
        "id": "ei4utd6jJiOJ"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = client.models.generate_content(\n",
        "    model = \"gemini-2.0-flash\",\n",
        "    config = short_config,\n",
        "    contents = \"Write a 1000 word essay on the importance of cats in ancient Egyptian society.\"\n",
        ")\n",
        "\n",
        "print(response.text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SFsjgQrLMCp7",
        "outputId": "ea095520-2e6f-4646-f8d9-2794281eb1fd"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "## The Divine Feline: Cats and their Significance in Ancient Egyptian Society\n",
            "\n",
            "The image of a sleek, elegant cat basking in the sun is almost synonymous with Ancient Egypt. Far from being mere pets, cats held a position of profound importance in this ancient society, woven into the very fabric of their religious beliefs, social structures, and daily lives. Their significance extended beyond simple utility; they were revered as protectors, symbols of grace and domesticity, and even worshipped as deities. Understanding the multifaceted role of cats in Ancient Egypt provides a valuable insight into the intricate tapestry of their beliefs and the deep connection they forged with the natural world.\n",
            "\n",
            "One of the most significant aspects of the cat's importance was its association with divinity. The most prominent feline goddess was Bastet, initially depicted as a lioness, representing ferocity and strength. As her cult evolved, she transitioned to a domestic cat or a woman with a cat's head, embodying gentler qualities like motherhood, fertility, and domestic harmony\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = client.models.generate_content(\n",
        "    model = \"gemini-2.0-flash\",\n",
        "    config = short_config,\n",
        "    contents = \"Write a short poem on the importance of cats in ancient Egypt.\"\n",
        ")\n",
        "\n",
        "print(response.text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JrwpCUxvMYLQ",
        "outputId": "c23fdef9-0351-4532-961e-1d56f1705319"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "In Egypt's dawn, where gods held sway,\n",
            "A creature sleek, began to play.\n",
            "Not just a pet, but sacred grace,\n",
            "A feline form, in holy place.\n",
            "\n",
            "Bastet's eyes, in emerald gleam,\n",
            "Protected home, a waking dream.\n",
            "From rodent plague and lurking ill,\n",
            "Her furry paws, the balance still.\n",
            "\n",
            "So Pharaohs bowed, and temples rose,\n",
            "For cats, a gift, where kindness flows.\n",
            "A vital thread, in ancient weave,\n",
            "The sacred cat, they did believe.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   **Temperature**: Temperature controls the degree of randomness in token selection. Higher temperatures result in a higher number of candidate tokens from which the next output token is selected, and can produce more diverse results, while lower temperatures have the opposite effect, such that a temperature of 0 results in greedy decoding which is selecting the most probable token at each step.\n",
        "\n"
      ],
      "metadata": {
        "id": "epctNJFzM6L-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "high_temperature_config = types.GenerateContentConfig(temperature=2.0)\n",
        "\n",
        "for _ in range(5):\n",
        "  response = client.models.generate_content(\n",
        "      model = \"gemini-2.0-flash\",\n",
        "      config = high_temperature_config,\n",
        "      contents = \"Suggest some creative cat names. Respond in a single word.\"\n",
        "  )\n",
        "  if response.text:\n",
        "    print(response.text, \"-\" * 25)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h3cHDs1eM1Hj",
        "outputId": "f972e4fb-3103-40f2-8b83-9252fea8b4c2"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pixel.\n",
            " -------------------------\n",
            "Spark.\n",
            " -------------------------\n",
            "*   Shadow\n",
            "*   Jasper\n",
            "*   Luna\n",
            "*   Mochi\n",
            "*   Cosmo\n",
            "*   Pepper\n",
            "*   Willow\n",
            "*   Zephyr\n",
            "*   Pixel\n",
            "*   Rumble\n",
            " -------------------------\n",
            "Whiskers\n",
            " -------------------------\n",
            "Spark.\n",
            " -------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "low_temperature_config = types.GenerateContentConfig(temperature=0.0)\n",
        "\n",
        "for _ in range(5):\n",
        "  response = client.models.generate_content(\n",
        "      model = \"gemini-2.0-flash\",\n",
        "      config = low_temperature_config,\n",
        "      contents = \"Suggest some creative cat names. Respond in a single word.\"\n",
        "  )\n",
        "\n",
        "  if response.text:\n",
        "    print(response.text, \"-\" * 25)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x2O9R8LZQ6dS",
        "outputId": "5a8a6783-d008-4586-e339-f4045bb41ce4"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Whiskers\n",
            " -------------------------\n",
            "Whiskers\n",
            " -------------------------\n",
            "Whiskers\n",
            " -------------------------\n",
            "Whiskers\n",
            " -------------------------\n",
            "Whiskers\n",
            " -------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   **Top P**: Like Temperature, Top P is also used to control the diversity of the model's output. Top P defines the probability threshold that, once cumulatively exceeded, tokens stop being selected as candidates. A Top P of 0 is equivalent to greedy decoding, and a Top P of 1 typically selects every token in the model's vocabulary.\n",
        "\n",
        "*   **Top K**: Top K is a positive integer that defines the number of most probable tokens from which to select the output token. A Top K of 1 selects a single token, performing greedy decoding.\n"
      ],
      "metadata": {
        "id": "HljUlq6wSBTY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_config = types.GenerateContentConfig(\n",
        "    # Default values for Gemini-2.0-flash\n",
        "    temperature = 1.0,\n",
        "    top_p = 0.95\n",
        ")\n",
        "\n",
        "story_prompt = \"You are a mystery writer. Write a short story about a cat who helps solve a murder mystery.\"\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model = \"gemini-2.0-flash\",\n",
        "    config = model_config,\n",
        "    contents = story_prompt\n",
        ")\n",
        "\n",
        "print(response.text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZGstOEHJR_7j",
        "outputId": "4cdf17f6-0662-44c1-ed5f-f23e163fb0c9"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The fog hung thick in the alley, clinging to the brick walls like a damp shroud. Rain slicked the cobblestones, reflecting the neon glow of the \"Blue Moon\" bar in distorted puddles. That's where I found him. Silas, they called him. A stray, sleek and black as midnight, with eyes that burned like amber.\n",
            "\n",
            "I’m Jasper Finch, private investigator. And lately, my cases had been drier than a desert bone. Then came Mrs. Abigail Hawthorne, found strangled in her lavishly decorated apartment. The police were stumped. The apartment was locked from the inside, no signs of forced entry, and a room full of priceless porcelain cats – Abigail’s pride and joy – undisturbed.\n",
            "\n",
            "\"It's a locked room miracle,\" Detective Miller had sighed, frustration etched on his face. I didn't believe in miracles. Only secrets.\n",
            "\n",
            "I took the case. Back at Abigail’s apartment, the scent of lilies, her favorite flower, still hung heavy in the air. The air felt stagnant, like the truth itself was suffocating. I examined everything: the silken Persian rugs, the ornate fireplace, the meticulously arranged collection of porcelain felines. Each held a silent, accusing gaze.\n",
            "\n",
            "That's when I noticed Silas, perched on the windowsill, his amber eyes fixed on a chipped ginger cat figurine on a nearby shelf. It was slightly askew, a tiny sliver of plaster missing from its ear.\n",
            "\n",
            "I picked it up. It felt lighter than it should. A hollow click echoed from within. Inside, tucked within the porcelain cavity, was a tightly folded piece of paper.\n",
            "\n",
            "It was a lottery ticket. A winning lottery ticket. Worth a small fortune.\n",
            "\n",
            "Now, Abigail Hawthorne was a wealthy woman. She didn’t need lottery winnings. But her nephew, Miles, did. Miles was a gambling addict, deep in debt and perpetually on the verge of ruin. He visited her regularly, always with a plea for money that she invariably refused.\n",
            "\n",
            "My mind raced. Miles knew about the lottery ticket, hidden within the porcelain cat. He came to retrieve it, and Abigail caught him. A struggle ensued. He strangled her, replaced the cat, locking the door behind him with a spare key he’d secretly obtained on a previous visit.\n",
            "\n",
            "It was a theory, but I needed proof. I brought Silas with me to Miles’ rundown apartment. The air reeked of stale cigarettes and desperation. Silas, usually aloof, became agitated, hissing and batting at a worn-out armchair.\n",
            "\n",
            "I examined the chair. Hidden beneath the cushion, a faint glimmer caught my eye. A small, silver locket, engraved with the initial \"A.\" It had been broken, the chain snapped. Abigail always wore it.\n",
            "\n",
            "The evidence was irrefutable. Miles confessed. He'd panicked. He’d never intended to kill her, he claimed, just scare her into giving him the ticket.\n",
            "\n",
            "Back in my office, I scratched Silas behind the ears. \"You have a knack for this, Silas,\" I said. \"A real nose for trouble.\"\n",
            "\n",
            "He purred, a low rumble that vibrated in the quiet room. I knew then that Silas was more than just a stray. He was my partner. A feline Watson to my weary Holmes. And in the fog-drenched alleys and shadowed corners of this city, that's exactly what I needed. The city was full of secrets, and Silas had a way of sniffing them out, one porcelain cat and one stolen locket at a time. My dry spell was over. I had a feeling things were about to get very interesting.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prompting\n",
        "\n",
        "\n",
        "\n",
        "*   **Zero-Shot Prompting**: Zero-Shot prompts are prompts that describe the request for the model directly.\n",
        "\n"
      ],
      "metadata": {
        "id": "W-nnNzTzTxxK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_config = types.GenerateContentConfig(\n",
        "    temperature = 0.1,\n",
        "    top_p = 1,\n",
        "    max_output_tokens = 5\n",
        ")\n",
        "\n",
        "zero_shot_prompt = \"\"\" Classify movie reviews as POSITIVE, NEUTRAL or NEGATIVE.\n",
        "Review: \"Her\" is a disturbing study revealing the direction\n",
        "humanity is headed if AI is allowed to keep evolving,\n",
        "unchecked. I wish there were more movies like this masterpiece.\n",
        "Sentiment: \"\"\"\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model = \"gemini-2.0-flash\",\n",
        "    config = model_config,\n",
        "    contents = zero_shot_prompt\n",
        ")\n",
        "\n",
        "print(response.text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5rZyYM6rTwQq",
        "outputId": "777ac63d-15f1-46c5-dfed-e093de400527"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "POSITIVE\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Enum mode\n",
        "\n",
        "The models are trained to generate text, and while the Gemini 2.0 models are great at following instructions, other models can sometimes produce more text than we may wish for. In the previous example, the model will output the label, but sometimes it can include a preceding \"Sentiment\" label, and without an output token limit, it may also add explanatory text afterwards."
      ],
      "metadata": {
        "id": "egPLc61MVNF3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import enum\n",
        "\n",
        "class Sentiment(enum.Enum):\n",
        "  POSITIVE = \"Positive\"\n",
        "  NEGATIVE = \"Negative\"\n",
        "  NEUTRAL = \"Neutral\"\n"
      ],
      "metadata": {
        "id": "aKC65_shVoXP"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = client.models.generate_content(\n",
        "    model = \"gemini-2.0-flash\",\n",
        "    config = types.GenerateContentConfig(\n",
        "        response_mime_type = \"text/x.enum\",\n",
        "        response_schema = Sentiment\n",
        "    ),\n",
        "    contents = zero_shot_prompt\n",
        ")\n",
        "\n",
        "print(response.text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7HIVsAWuV8JD",
        "outputId": "2b12ed8d-1f4d-4806-b87e-5ceefa44bb62"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Positive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "When using constrained output like an enum, the Python SDK will attempt to convert the model's text response into a Python object automatically. It's stored in the `response.parsed` field."
      ],
      "metadata": {
        "id": "QCOqOtAuaS3q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "enum_response = response.parsed\n",
        "print(enum_response)\n",
        "print(type(enum_response))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fWoTOmqUaPql",
        "outputId": "2df8b0ea-88ed-4872-a8bb-5e470596ba63"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentiment.POSITIVE\n",
            "<enum 'Sentiment'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   **One-Shot Prompting and Few-Shot Prompting**: Providing an example of the expected response is known as a \"One-Shot\" prompt. When we provide multiple examples, it is a \"Few-Shot\" prompt.\n"
      ],
      "metadata": {
        "id": "lcAK7WfIa6xv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "few_shot_prompt = \"\"\"Parse a customer's pizza order into valid JSON:\n",
        "\n",
        "EXAMPLE:\n",
        "I want a small pizza with cheese, tomato sauce, and pepperoni.\n",
        "JSON Response:\n",
        "```\n",
        "{\n",
        "\"size\": \"small\",\n",
        "\"type\": \"normal\",\n",
        "\"ingredients\": [\"cheese\", \"tomato sauce\", \"pepperoni\"]\n",
        "}\n",
        "```\n",
        "\n",
        "EXAMPLE:\n",
        "Can I get a large pizza with tomato sauce, basil and mozzarella\n",
        "JSON Response:\n",
        "```\n",
        "{\n",
        "\"size\": \"large\",\n",
        "\"type\": \"normal\",\n",
        "\"ingredients\": [\"tomato sauce\", \"basil\", \"mozzarella\"]\n",
        "}\n",
        "```\n",
        "\n",
        "ORDER:\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "id": "brv5Q1pOayzU"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "customer_order = \"Give me a large with cheese & pineapple.\"\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model = \"gemini-2.0-flash\",\n",
        "    config = types.GenerateContentConfig(\n",
        "        temperature = 0.1,\n",
        "        top_p = 1,\n",
        "        max_output_tokens = 250\n",
        "    ),\n",
        "    contents = [few_shot_prompt, customer_order]\n",
        ")\n",
        "\n",
        "print(response.text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DNq1i_ZbbZFT",
        "outputId": "7c3874e3-f266-4ff2-884c-0847bcea2ef9"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "```json\n",
            "{\n",
            "\"size\": \"large\",\n",
            "\"type\": \"normal\",\n",
            "\"ingredients\": [\"cheese\", \"pineapple\"]\n",
            "}\n",
            "```\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### JSON mode\n",
        "\n",
        "To provide control over the schema, and to ensure that you only receive JSON (with no other text or markdown), you can use the Gemini API's <a href=\"https://github.com/google-gemini/cookbook/blob/main/quickstarts/JSON_mode.ipynb\">JSON mode</a>. This forces the model to constrain decoding, such that token selection is guided by the supplied schema."
      ],
      "metadata": {
        "id": "pWsffkjNdGJ_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import typing_extensions as typing\n",
        "\n",
        "class PizzaOrder(typing.TypedDict):\n",
        "  size: str\n",
        "  ingredients: list[str]\n",
        "  type: str\n"
      ],
      "metadata": {
        "id": "bsXwwCcIdAsd"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = client.models.generate_content(\n",
        "    model = \"gemini-2.0-flash\",\n",
        "    config = types.GenerateContentConfig(\n",
        "        temperature = 0.1,\n",
        "        response_mime_type = \"application/json\",\n",
        "        response_schema = PizzaOrder\n",
        "    ),\n",
        "    contents = \"Can I have a large dessert pizza with apple and chocolate?\"\n",
        ")\n",
        "\n",
        "print(response.text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YKfpVPAOdkYm",
        "outputId": "1a03e2d5-5612-4a2a-d79f-dee8c53c36f5"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  \"size\": \"large\",\n",
            "  \"ingredients\": [\"apple\", \"chocolate\"],\n",
            "  \"type\": \"dessert\"\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   **Chain-of-Thought**: Direct prompting on LLMs can return answers quickly and (in terms of output token usage) efficiently, but they can be prone to hallucination. The answer may \"look\" correct (in terms of language and syntax) but is incorrect in terms of factuality and reasoning. Chain-of-Thought prompting is a technique where you instruct the model to output intermediate reasoning steps, and it typically gets better results, especially when combined with few-shot examples. It is worth noting that this technique doesn't completely eliminate hallucinations, and that it tends to cost more to run, due to the increased token count. Models like the Gemini family are trained to be \"chatty\" or \"thoughtful\" and will provide reasoning steps without prompting, so for this simple example you can ask the model to be more direct in the prompt to force a non-reasoning response. Try re-running this step if the model gets lucky and gets the answer correct on the first try."
      ],
      "metadata": {
        "id": "Zj97FlsGpIyc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\"When I was 4 years old, my sister was 3 times my age. Now, I\n",
        "am 20 years old. How old is my sister? Return the answer directly.\"\"\"\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model = \"gemini-2.0-flash\",\n",
        "    contents = prompt\n",
        ")\n",
        "\n",
        "print(response.text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bSClh2L-pDL5",
        "outputId": "1448b026-f056-46c6-dfba-db73ce152a03"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "23\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, trying the same approach, but indicating to the model that it should \"think step by step\"."
      ],
      "metadata": {
        "id": "It7hrMYZqAEG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\"When I was 4 years old, my sister was 3 times my age. Now, I\n",
        "am 20 years old. How old is my sister? Let's think step by step.\"\"\"\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model = \"gemini-2.0-flash\",\n",
        "    contents = prompt\n",
        ")\n",
        "\n",
        "Markdown(response.text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        },
        "id": "9LOS7FrvqDUl",
        "outputId": "0a0ddb92-9791-4f8e-dc9f-7eeaa532e1e8"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Here's how to solve the problem:\n\n1. **Find the age difference:** When you were 4, your sister was 3 * 4 = 12 years old. The age difference between you and your sister is 12 - 4 = 8 years.\n\n2. **Apply the age difference:** Since the age difference remains constant, your sister is always 8 years older than you.\n\n3. **Calculate your sister's current age:** Now that you are 20, your sister is 20 + 8 = 28 years old.\n\n**Answer:** Your sister is 28 years old.\n"
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ReAct: Reason and Act\n",
        "\n",
        "In the following example, we will run a ReAct prompt directly in the Gemini API and perform the searching steps ourselves. As this prompt follows a well-defined structure, there are frameworks available that wrap the prompt into easier-to-use APIs that make tool calls automatically, such as the LangChain example from the \"Prompting\" whitepaper.\n"
      ],
      "metadata": {
        "id": "o4Hyiq_Aqbif"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_instructions = \"\"\"\n",
        "Solve a question answering task with interleaving Thought, Action, Observation steps. Thought can reason about the current situation,\n",
        "Observation is understanding relevant information from an Action's output and Action can be one of three types:\n",
        " (1) <search>entity</search>, which searches the exact entity on Wikipedia and returns the first paragraph if it exists. If not, it\n",
        "     will return some similar entities to search and you can try to search the information from those topics.\n",
        " (2) <lookup>keyword</lookup>, which returns the next sentence containing keyword in the current context. This only does exact matches,\n",
        "     so keep your searches short.\n",
        " (3) <finish>answer</finish>, which returns the answer and finishes the task.\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "id": "Npv0TeL-qYsh"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example1 = \"\"\"Question\n",
        "Musician and satirist Allie Goertz wrote a song about the \"The Simpsons\" character Milhouse, who Matt Groening named after who?\n",
        "\n",
        "Thought 1\n",
        "The question simplifies to \"The Simpsons\" character Milhouse is named after who. I only need to search Milhouse and find who it is named after.\n",
        "\n",
        "Action 1\n",
        "<search>Milhouse</search>\n",
        "\n",
        "Observation 1\n",
        "Milhouse Mussolini Van Houten is a recurring character in the Fox animated television series The Simpsons voiced by Pamela Hayden and created by Matt Groening.\n",
        "\n",
        "Thought 2\n",
        "The paragraph does not tell who Milhouse is named after, maybe I can look up \"named after\".\n",
        "\n",
        "Action 2\n",
        "<lookup>named after</lookup>\n",
        "\n",
        "Observation 2\n",
        "Milhouse was named after U.S. president Richard Nixon, whose middle name was Milhous.\n",
        "\n",
        "Thought 3\n",
        "Milhouse was named after U.S. president Richard Nixon, so the answer is Richard Nixon.\n",
        "\n",
        "Action 3\n",
        "<finish>Richard Nixon</finish>\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "id": "QriScw_BrHdI"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example2 = \"\"\"Question\n",
        "What is the elevation range for the area that the eastern sector of the Colorado orogeny extends into?\n",
        "\n",
        "Thought 1\n",
        "I need to search Colorado orogeny, find the area that the eastern sector of the Colorado orogeny extends into, then find the elevation range of the area.\n",
        "\n",
        "Action 1\n",
        "<search>Colorado orogeny</search>\n",
        "\n",
        "Observation 1\n",
        "The Colorado orogeny was an episode of mountain building (an orogeny) in Colorado and surrounding areas.\n",
        "\n",
        "Thought 2\n",
        "It does not mention the eastern sector. So I need to look up eastern sector.\n",
        "\n",
        "Action 2\n",
        "<lookup>eastern sector</lookup>\n",
        "\n",
        "Observation 2\n",
        "The eastern sector extends into the High Plains and is called the Central Plains orogeny.\n",
        "\n",
        "Thought 3\n",
        "The eastern sector of Colorado orogeny extends into the High Plains. So I need to search High Plains and find its elevation range.\n",
        "\n",
        "Action 3\n",
        "<search>High Plains</search>\n",
        "\n",
        "Observation 3\n",
        "High Plains refers to one of two distinct land regions\n",
        "\n",
        "Thought 4\n",
        "I need to instead search High Plains (United States).\n",
        "\n",
        "Action 4\n",
        "<search>High Plains (United States)</search>\n",
        "\n",
        "Observation 4\n",
        "The High Plains are a subregion of the Great Plains. From east to west, the High Plains rise in elevation from around 1,800 to 7,000 ft (550 to 2,130m).\n",
        "\n",
        "Thought 5\n",
        "High Plains rise in elevation from around 1,800 to 7,000 ft, so the answer is 1,800 to 7,000 ft.\n",
        "\n",
        "Action 5\n",
        "<finish>1,800 to 7,000 ft</finish>\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "id": "d62YnhS6rIlY"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To capture a single step at a time, while ignoring any hallucinated Observation steps, we will use `stop_sequences` to end the generation process. The steps are Thought, Action, Observation, in that order."
      ],
      "metadata": {
        "id": "-dvyaKJqrcZW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"\"\"Question\n",
        "Who was the youngest author listed on the transformers NLP paper?\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "id": "AD5qWNDMrWpR"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# You will perform the Action; so generate up to, but not including, the Observation.\n",
        "\n",
        "react_config = types.GenerateContentConfig(\n",
        "    stop_sequences = [\"\\nObservation\"],\n",
        "    system_instruction = model_instructions + example1 + example2,\n",
        ")\n"
      ],
      "metadata": {
        "id": "-oEtik2huxgT"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a chat that has the model instructions and examples pre-seeded.\n",
        "\n",
        "react_chat = client.chats.create(\n",
        "    model = 'gemini-2.0-flash',\n",
        "    config = react_config,\n",
        ")\n"
      ],
      "metadata": {
        "id": "k4l9F-VEu8eX"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = react_chat.send_message(question)\n",
        "print(response.text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dz0PFzQXxSTD",
        "outputId": "db1a073e-4cb1-438e-c5d2-ff83c9cc3c60"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thought 1\n",
            "I need to find the transformers NLP paper and then look for the list of authors to determine the youngest author.\n",
            "\n",
            "Action 1\n",
            "<search>transformers NLP paper</search>\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we can perform this research ourselves and supply it back to the model."
      ],
      "metadata": {
        "id": "Y1y7G9cgxdBw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "observation = \"\"\"Observation 1\n",
        "[1706.03762] Attention Is All You Need\n",
        "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin\n",
        "We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely.\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "id": "V0ujcyPtxf5S"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = react_chat.send_message(observation)\n",
        "print(response.text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CRoMHD0CxkVY",
        "outputId": "248f81f5-702a-4e76-f87e-ad01b129e685"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thought 2\n",
            "I have the list of authors: Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin. I need to figure out their ages when the paper was published in 2017. This is difficult to find so I will search for Aidan N. Gomez as he seems to be the youngest based on the name.\n",
            "\n",
            "Action 2\n",
            "<search>Aidan N. Gomez</search>\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This process repeats until the `<finish>` action is reached."
      ],
      "metadata": {
        "id": "-n5Vw4ckx4_O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Thinking mode\n",
        "\n",
        "The experiemental Gemini Flash 2.0 \"Thinking\" model has been trained to generate the \"thinking process\" the model goes through as part of its response. As a result, the Flash Thinking model is capable of stronger reasoning capabilities in its responses.\n",
        "\n",
        "Using a \"thinking mode\" model can provide us with high-quality responses without needing specialised prompting like the previous approaches. One reason this technique is effective is that we induce the model to generate relevant information (\"brainstorming\", or \"thoughts\") that is then used as part of the context in which the final response is generated."
      ],
      "metadata": {
        "id": "oJsUiSWTx8bO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import io\n",
        "from IPython.display import Markdown, clear_output\n"
      ],
      "metadata": {
        "id": "W8ATLHh9xsW6"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = client.models.generate_content_stream(\n",
        "    model = \"gemini-2.0-flash-thinking-exp\",\n",
        "    contents = \"Who was the youngest author listed on the transformers NLP paper?\",\n",
        ")\n"
      ],
      "metadata": {
        "id": "VW0fnIC3yUls"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "buffer_ = io.StringIO()\n",
        "\n",
        "for chunk in response:\n",
        "  buffer_.write(chunk.text)\n",
        "\n",
        "  # Displaying the response as it is streamed\n",
        "  print(chunk.text, end = \"\")\n",
        "\n",
        "# Rendering the finished response as formatted Markdown\n",
        "\n",
        "clear_output()\n",
        "Markdown(buffer_.getvalue())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145
        },
        "id": "wOx9OzZRyhKv",
        "outputId": "27d3a318-866c-4cbd-b942-1cedcef1b00a"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "The youngest author listed on the \"Attention is All You Need\" paper (which introduced the Transformer architecture and is widely considered *the* foundational \"transformers NLP paper\") is **Aidan N. Gomez**.\n\nAt the time of the paper's publication in 2017, Aidan N. Gomez was a PhD student at the University of Toronto, supervised by Geoffrey Hinton.  The other authors were more senior researchers, primarily from Google Brain.\n\nWhile we don't have publicly available birthdates for all authors to definitively confirm the absolute youngest, Aidan N. Gomez's status as a PhD student at the time strongly indicates he was the youngest author on the paper."
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Code prompting\n",
        "\n",
        "### Generating code\n",
        "The Gemini family of models can be used to generate code, configuration and scripts. Generating code can be helpful when learning to code, learning a new language or for rapidly generating a first draft. It's important to be aware that since LLMs can make mistakes, and can repeat training data, it's essential to read and test your code first, and comply with any relevant licenses.\n",
        "\n"
      ],
      "metadata": {
        "id": "Ub3o_B-UzemQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "code_prompt = \"\"\"\n",
        "Write a Python function to calculate the factorial of a number. No explanation, provide only the code.\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "id": "QVM133QizWkm"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = client.models.generate_content(\n",
        "    model = \"gemini-2.0-flash\",\n",
        "    config = types.GenerateContentConfig(\n",
        "        temperature = 1,\n",
        "        top_p = 1,\n",
        "        max_output_tokens = 1024,\n",
        "    ),\n",
        "    contents = code_prompt\n",
        ")\n",
        "\n",
        "Markdown(response.text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 451
        },
        "id": "FLXiHvXAzum5",
        "outputId": "862a9687-32a6-465d-c073-cfa5d68fa8de"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "```python\ndef factorial(n):\n  \"\"\"\n  Calculates the factorial of a non-negative integer.\n\n  Args:\n    n: A non-negative integer.\n\n  Returns:\n    The factorial of n, or 1 if n is 0.\n\n  Raises:\n    TypeError: if n is not an integer.\n    ValueError: if n is a negative integer.\n  \"\"\"\n  if not isinstance(n, int):\n    raise TypeError(\"Input must be an integer\")\n  if n < 0:\n    raise ValueError(\"Input must be a non-negative integer\")\n  if n == 0:\n    return 1\n  else:\n    result = 1\n    for i in range(1, n + 1):\n      result *= i\n    return result\n```\n"
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Code execution\n"
      ],
      "metadata": {
        "id": "jt2ul4aB0Gxv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pprint import pprint\n",
        "\n",
        "config = types.GenerateContentConfig(\n",
        "    tools = [types.Tool(code_execution = types.ToolCodeExecution())]\n",
        ")\n",
        "\n",
        "code_execution_prompt = \"\"\"\n",
        "Generate the first 14 odd prime numbers, then calculate their sum.\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "id": "HqAbPqnd0FWp"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = client.models.generate_content(\n",
        "    model = \"gemini-2.0-flash\",\n",
        "    config = config,\n",
        "    contents = code_execution_prompt\n",
        ")\n"
      ],
      "metadata": {
        "id": "Yw_6ycmc0gjN"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for part in response.candidates[0].content.parts:\n",
        "  pprint(part.to_json_dict()) # Extracts and prints each part of the AI-generated response in JSON format. Useful for debugging and understanding how the AI's response is structured.\n",
        "  print(\"-----\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SqTdJdjI0uiU",
        "outputId": "49865c02-c59a-464f-db4b-fcbc62239229"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'text': \"Okay, I can do that. First, I'll list the first 14 odd prime \"\n",
            "         \"numbers, and then I'll calculate their sum using a python code \"\n",
            "         'block.\\n'\n",
            "         '\\n'\n",
            "         'The first few prime numbers are 2, 3, 5, 7, 11, 13, 17, 19, 23, 29, '\n",
            "         '31, 37, 41, 43, 47, 53... Since we want *odd* primes, we exclude 2. '\n",
            "         'Therefore, the first 14 odd prime numbers are 3, 5, 7, 11, 13, 17, '\n",
            "         '19, 23, 29, 31, 37, 41, 43, 47.\\n'\n",
            "         '\\n'\n",
            "         \"Now, let's calculate their sum:\\n\"\n",
            "         '\\n'}\n",
            "-----\n",
            "{'executable_code': {'code': 'primes = [3, 5, 7, 11, 13, 17, 19, 23, 29, 31, '\n",
            "                             '37, 41, 43, 47]\\n'\n",
            "                             'total = sum(primes)\\n'\n",
            "                             'print(total)\\n',\n",
            "                     'language': 'PYTHON'}}\n",
            "-----\n",
            "{'code_execution_result': {'outcome': 'OUTCOME_OK', 'output': '326\\n'}}\n",
            "-----\n",
            "{'text': 'The sum of the first 14 odd prime numbers (3, 5, 7, 11, 13, 17, 19, '\n",
            "         '23, 29, 31, 37, 41, 43, 47) is 326.\\n'}\n",
            "-----\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This response contains multiple parts, including an opening and closing `text` part that represent regular responses, an `executable_code` part that represents generated code and a `code_execution_result` part that represents the results from running the generated code.\n",
        "\n",
        "Exploring them individually,"
      ],
      "metadata": {
        "id": "7gI6lA-r10w-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for part in response.candidates[0].content.parts:\n",
        "  if part.text:\n",
        "    display(Markdown(part.text))\n",
        "  elif part.executable_code:\n",
        "    display(Markdown(f'```Python\\n{part.executable_code.code}\\n```'))\n",
        "  elif part.code_execution_result:\n",
        "    if part.code_execution_result.outcome != 'OUTCOME_OK':\n",
        "      display(Markdown(f' ## Status {part.code_execution_result.outcome}'))\n",
        "    display(Markdown(f'```\\n{part.code_execution_result.output}\\n```'))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 209
        },
        "id": "sHdrGPnk1bgL",
        "outputId": "29513bac-9e5d-4c51-e5d6-926a19b30125"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Okay, I can do that. First, I'll list the first 14 odd prime numbers, and then I'll calculate their sum using a python code block.\n\nThe first few prime numbers are 2, 3, 5, 7, 11, 13, 17, 19, 23, 29, 31, 37, 41, 43, 47, 53... Since we want *odd* primes, we exclude 2. Therefore, the first 14 odd prime numbers are 3, 5, 7, 11, 13, 17, 19, 23, 29, 31, 37, 41, 43, 47.\n\nNow, let's calculate their sum:\n\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "```Python\nprimes = [3, 5, 7, 11, 13, 17, 19, 23, 29, 31, 37, 41, 43, 47]\ntotal = sum(primes)\nprint(total)\n\n```"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "```\n326\n\n```"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "The sum of the first 14 odd prime numbers (3, 5, 7, 11, 13, 17, 19, 23, 29, 31, 37, 41, 43, 47) is 326.\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Explaining code\n",
        "The Gemini family of models can explain code as well. In the following example, we will pass a bash script and ask some questions."
      ],
      "metadata": {
        "id": "cUJ-bDGj4Jdj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "file_contents = !curl https://raw.githubusercontent.com/magicmonty/bash-git-prompt/refs/heads/master/gitprompt.sh\n"
      ],
      "metadata": {
        "id": "VTVlxMgF4LHS"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "explain_code_prompt = f\"\"\"\n",
        "Please explain what this file does at a very high level. What is it, and why would I use it?\n",
        "\n",
        "```\n",
        "{file_contents}\n",
        "```\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "id": "ptVR7l4i4mKK"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = client.models.generate_content(\n",
        "    model = \"gemini-2.0-flash\",\n",
        "    contents = explain_code_prompt,\n",
        ")\n",
        "\n",
        "Markdown(response.text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 198
        },
        "id": "clMi6O2x4pSa",
        "outputId": "c04a973c-27a5-4675-b9d1-c674e8c389bd"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "This file is a script designed to enhance your command-line prompt in Bash or Zsh, specifically when working with Git repositories.  In a nutshell, it makes your prompt display information about the current Git repository, such as the branch name, the status of changes (staged, unstaged, untracked), and whether the local branch is ahead or behind the remote branch.  It also allows for customization of the prompt's appearance through themes and configuration options.\n\nHere's a breakdown of why you'd use it:\n\n*   **Git Status at a Glance:**  Instead of having to type `git status` repeatedly, the prompt displays essential information, saving you time and effort.\n*   **Improved Workflow:** By providing immediate feedback on the state of your repository, it can help you avoid common mistakes and stay organized.\n*   **Customization:** You can tailor the prompt's appearance (colors, symbols, displayed information) to your personal preferences.\n*   **Virtual Environment Awareness:** It displays the currently active virtual environment, useful for Python or Node.js development.\n*   **Cross-Shell Compatibility:** Aims to work reasonably well in both Bash and Zsh.\n"
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    }
  ]
}